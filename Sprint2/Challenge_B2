{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Challenge_B2","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPQbxdY0ZMZDjtv9TPenol3"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"B_JpdfO35CGx"},"source":["# Bitacora 19-20\n","\n","**Ejercicio 1 - Challenge 1:** Comienza con una estrategia sencilla, imputar con el valor medio de cada columna usando Pandas. \n","\n","¿Para cuáles columnas tendrá sentido esto, teniendo en cuenta lo visto en el punto anterior? \n","Para aquellas columnas numericas V3, V4 y V5.\n","\n","\n","Por las dudas, comenzamos volviendo a cargar los datos:\n","\n"]},{"cell_type":"code","metadata":{"id":"WaRhi24749iz"},"source":["import numpy as np\n","import pandas as pd \n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","\n","\n","\n","from google.colab import drive\n","drive.mount('/content/drive') #monto el drive en collab y leo el file \"DS_Proyecto_01_Datos_Properati.csv\" insertando la url con extension csv\n","\n","ValorAtipico1=pd.read_csv('/content/drive/My Drive/Colab Notebooks/DS_Bitácora_19_Data_VA1.csv', sep=',',engine='python')\n","ValorAtipico2=pd.read_csv('/content/drive/My Drive/Colab Notebooks/DS_Bitácora_19_Data_VA2.csv', sep=',',engine='python')\n","ValorFaltante=pd.read_csv('/content/drive/My Drive/Colab Notebooks/DS_Bitácora_19_Data_con_VF.csv', sep=',',engine='python')\n","Valor_distrofia=pd.read_csv('/content/drive/My Drive/Colab Notebooks/DS_Bitácora_19_Distrofia.csv', sep=',',engine='python')\n","\n","\n","dataVF = ValorFaltante.copy()\n","\n","print(dataVF)\n","datasinVF=dataVF.dropna()\n","print(datasinVF)\n","\n","#forma intuitiva es llenar con la media\n","dataVF1 = ValorFaltante.copy()\n","dataVF1.fillna(dataVF1.mean(),inplace=True) # es independiente del lugar que veo. supongo que cualquier valor faltante reemplace con la media (vale si la desviacion es pequeña y una distribucion normal es bueno, porque los datos son muy parecidos) si tiene curtosis alta, muy aplanada no es bueno\n","\n","#histogramas de las variables con y sin valores faltantes.\n","dataVF[[\"V2\", \"V3\",'V4','V5']].plot.hist(alpha=0.4, bins=100) \n","datasinVF[[\"V2\", \"V3\",'V4','V5']].plot.hist(alpha=0.4, bins=100) \n","\n","sns.pairplot(dataVF1,hue='V1', hue_order=['A','B','C']) \n","# como impute los valores faltantes por la media, veo la distribuin más ´picuda\" entonces"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zObdk-1-7iLn"},"source":["#inputo los valores faltantes con la media de cada columna\n","print((dataVF.isna()).sum())\n","\n","#---- Selective Treatment----------------\n","for i in dataVF.columns[dataVF.isnull().any(axis=0)]:     #---Applying Only on variables with NaN values\n","    dataVF[i].fillna(dataVF[i].mean(),inplace=True)\n","\n","print((dataVF.isna()).sum())\n","\n","plt.figure()\n","Dat=dataVF.isna()\n","heat=sns.heatmap(Dat.T,  cmap=\"YlGnBu\",xticklabels=False)\n","plt.title('Datos Faltantes (=1) dentro del Dataset')\n","plt.xlabel('Instancias dentro del Dataset')\n","plt.ylabel('Elementos columna del Dataset')\n","plt.tight_layout()\n","plt.show()\n","\n","#Al imputar, ¿cambián las distribuciones?¿Esto está bien o mal?\n","dataVF[[\"V2\", \"V3\",'V4','V5']].plot.hist(alpha=0.4, bins=100) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sj47fG2N-LqT"},"source":["## 3. Escalado de datos - Challenge 2\n","\n","Seguiremos usando los datos de la sección anterior:\n","\n","1. Agregar al dataframe una columna que se llame \"Datos_Reescalados\" y que contenga los datos reescalados por Z-Score. ¿Cuál es el valor medio de los datos reescalados?¿Y su desviación estándar?\n","\n"," **Nota:** Para reescalar los datos, pueden hacerlo \"a mano\" o utilizar la clase `StandardScaler` de Scikit-Learn. No te olvides que las herramientas de preprocesamiento de datos de Scikit-learn tienen los mismos métodos (crear el objeto, fitearlo, transfromar los datos). Si necesitás ayuda, podés consultar la [documentación](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n","2. Realizá, en un mismo gráfico, un histograma de los datos reescaleados y un histograma de los datos crudos. ¿Qué similitudes y qué diferencias tiene con la distribución de datos crudos?\n","3. Aplica la regla de las tres sigmas utilizando los datos reescalados."]},{"cell_type":"code","metadata":{"id":"2YpHonTX-MmN"},"source":["#1. Agregar al dataframe una columna que se llame \"Datos_Reescalados\" y que contenga los datos reescalados por Z-Score.\n","# ¿Cuál es el valor medio de los datos reescalados?¿Y su desviación estándar?\n","#los datos reescalados idealmente tienen una media poblacional cero y un desvio de 1.\n","\n","dataVa1=ValorAtipico1.copy()\n","dataVa2=ValorAtipico2.copy()\n","\n","#reescalo con la normalización(estandarizo el valor x-mu/sigma) a mano\n","valor_medio = dataVa1.mean()  #dataVa1.Datos.mean()\n","std = dataVa1.std() #dataVa1.Datos.std()\n","print('el valor medio de mis datos es {} y la desviacion estandar {}'.format(float(valor_medio),float(std)))\n","dataVa1['Datos_Reescalados'] = (dataVa1-valor_medio)/std\n","print(dataVa1.head())\n","print('la media del dato estandarizado {} y el desvio {}'.format(dataVa1['Datos_Reescalados'].mean(),dataVa1['Datos_Reescalados'].std()))\n","\n","print()\n","#reescalo con la normalización(estandarizo el valor x-mu/sigma)\n","valor_medio = dataVa2.mean()\n","std = dataVa2.std()\n","print('el valor medio {} y la desviacion estandar {}'.format(float(valor_medio),float(std)))\n","dataVa2['Datos_Reescalados'] = (dataVa2-valor_medio)/std\n","print(dataVa2.head())\n","print('la media del dato estandarizado {} y el desvio {}'.format(dataVa2['Datos_Reescalados'].mean(),dataVa2['Datos_Reescalados'].std()))\n","#cada dato esta +-X alejado en desvios estandar respecto de la media. es decir si el valor es -1,28 esta a -1,28 desivios estandar respecto de la media (el desvio es 1 aca).\n","\n","\n","#normalized_df=(df-df.mean())/df.std()\n","#normalized_df=(df-df.min())/(df.max()-df.min()\n","\n","\n","\n","#data.Datos.hist(label='Datos crudos')\n","#data.Datos_Reescalados.hist(label='reescalados)\n","#plt.legend\n","#plt.show()\n","\n","#regla de 3 sigma\n","#mascara_outlier=np.logical_or(Datos_reescaldos<-3,Datos_reescaldos>3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I2cD1fQK-P7t"},"source":["#lo hago con sklearn\n","#Nota: Para reescalar los datos, pueden hacerlo \"a mano\" o utilizar la clase StandardScaler de Scikit-Learn. \n","#No te olvides que las herramientas de preprocesamiento de datos de Scikit-learn tienen los mismos métodos (crear el objeto, fitearlo, transfromar los datos). \n","#https://scikit-learn.org/stable/auto_examples/release_highlights/plot_release_highlights_0_23_0.html#sphx-glr-auto-examples-release-highlights-plot-release-highlights-0-23-0-py\n","#https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n","\n","import pandas as pd\n","from sklearn import preprocessing\n","dataVa1=ValorAtipico1.copy()\n","dataVa2=ValorAtipico2.copy()\n","\n","x = dataVa1.values #returns a numpy array\n","STscaler = preprocessing.StandardScaler()\n","x_STscaled = STscaler.fit_transform(x)\n","dataVa1['Datos_Reescalados_estandarizado'] =pd.DataFrame(x_STscaled)\n","\n","min_max_scaler = preprocessing.MinMaxScaler()\n","x_scaledminmax = min_max_scaler.fit_transform(x)\n","dataVa1['Datos_Reescalados_minmax'] =pd.DataFrame(x_scaledminmax)\n","\n","print(dataVa1)\n","\n","x = dataVa2.values #returns a numpy array\n","STscaler = preprocessing.StandardScaler()\n","x_STscaled = STscaler.fit_transform(x)\n","dataVa2['Datos_Reescalados_estandarizado'] =pd.DataFrame(x_STscaled)\n","\n","min_max_scaler = preprocessing.MinMaxScaler()\n","x_scaledminmax = min_max_scaler.fit_transform(x)\n","dataVa2['Datos_Reescalados_minmax'] =pd.DataFrame(x_scaledminmax)\n","\n","print(dataVa2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TgbzfO3v-R7x"},"source":["#Realizá, en un mismo gráfico, un histograma de los datos reescaleados y un histograma de los datos crudos. \n","#¿Qué similitudes y qué diferencias tiene con la distribución de datos crudos?\n","plt.figure()\n","sns.distplot(dataVa1['Datos'])\n","sns.distplot(dataVa1['Datos_Reescalados_estandarizado'])\n","\n","plt.figure()\n","sns.distplot(dataVa2['Datos'])\n","sns.distplot(dataVa2['Datos_Reescalados_estandarizado'])\n","\n","#se modifica la curtosis, la forma de la distribución es casisimilar. hay más datos en el centro."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0A4ip7U--UCn"},"source":["#Aplica la regla de las tres sigmas utilizando los datos reescalados.\n","#la regla de las tres sigmas \n","def OutlierSD(X,Atributo):\n","     \"\"\"\n","      Se considera que los valores Outliers se obtienen a partir de los valores:\n","      Por encima de media + 3 * SD y por debajo de media - 3 * SD.\n","      Ante eso se buscan los valores que son outliers inferior y superior de mi dataset en algun atributo en particular.\n","      1ro se calculan el promedio y la desviacion estandar.\n","      2do se calculan los valores para delimitar los extremos.\n","     \"\"\"\n","     #Calculo los limites para cada tipo de propiedad principal para contar y filtrar los outliers\n","     mean=X[Atributo].mean()\n","     S=X[Atributo].std()\n","     L_s_inf= mean-3*S\n","     L_s_sup= mean+3*S\n","     return float(L_s_inf), float(L_s_sup)\n","\n","\n","L_inf_s_a1 ,L_sup_s_a1= OutlierSD(dataVa1,\"Datos_Reescalados_estandarizado\")\n","L_inf_s_a2 ,L_sup_s_a2= OutlierSD(dataVa2,\"Datos_Reescalados_estandarizado\")\n","print('Los valores de los límites extremos (L_inf,L_sup) de V1: '+str((L_inf_s_a1,L_sup_s_a1))+' y V2: ' +str((L_inf_s_a2,L_sup_s_a2)))\n","\n","print('La cantidad de outliers sup de V1: '+str((dataVa1.Datos_Reescalados_estandarizado>L_sup_s_a1).sum()) +' y V2: ',str((dataVa2.Datos_Reescalados_estandarizado>L_sup_s_a2).sum()))\n","print('La cantidad de outliers inf de V1: '+str((dataVa1.Datos_Reescalados_estandarizado<L_inf_s_a1).sum()) +' y V2: ',str((dataVa2.Datos_Reescalados_estandarizado<L_inf_s_a2).sum()))\n","\n","sns.boxplot(x=dataVa1[\"Datos_Reescalados_estandarizado\"],notch=True, whis=3)\n","sns.boxplot(x=dataVa2[\"Datos_Reescalados_estandarizado\"],notch=True, whis=3)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cxPHAuoc-WOJ"},"source":["\n","ValorAtipico1_woOut_SD=dataVa1[dataVa1.Datos_Reescalados_estandarizado < L_sup_s_a1]\n","ValorAtipico1_woOut_SD=ValorAtipico1_woOut_SD[ValorAtipico1_woOut_SD.Datos_Reescalados_estandarizado > L_inf_s_a1]\n","\n","ValorAtipico2_woOut_SD=dataVa2[dataVa2.Datos_Reescalados_estandarizado < L_sup_s_a2]\n","ValorAtipico2_woOut_SD=ValorAtipico2_woOut_SD[ValorAtipico2_woOut_SD.Datos_Reescalados_estandarizado > L_inf_s_a2]\n","\n","\n","plt.figure()\n","sns.distplot(dataVa1['Datos'])\n","sns.distplot(dataVa1['Datos_Reescalados_estandarizado'])\n","sns.distplot(ValorAtipico1_woOut_SD['Datos_Reescalados_estandarizado'])\n","plt.figure()\n","sns.distplot(dataVa2['Datos'])\n","sns.distplot(dataVa2['Datos_Reescalados_estandarizado'])\n","sns.distplot(ValorAtipico2_woOut_SD['Datos_Reescalados_estandarizado'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xe0_075M-aJj"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"51Wz0Py8-aFS"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UqhMoXQ07ino"},"source":["#Encuentro 21 challenge\n","\n","Para pensar e investigar. Existen dataset desbalanceados, es decir, conjuntos de datos donde hay clases sobrerrepresentadas y otras subrepresentadas. Por ejemplo, el dataset de Iris es un dataset perfectamente balanceado (50 instancias de cada clase), mientras que el dataset del Titanic está levemente desbalanceado. Trabajar con datasets muy desbalanceados no es tarea sencilla, y existen muchas técnicas para abordarlos. \n","\n","Reflexiona sobre qué ocurre con la exactitud, la precisión y la exhaustividad cuando las utilizas para evaluar desempeño sobre dataset binario muy desbalanceado (supongamos, proporción 1 en 1000)\n","\n","    ->Si los datos están desbalanceado quiere decir que el conjunto de datos no es homogéneo y tiende a reinar una categoría por sobre otra, por lo que la muestra puede estar sesgada respecto a la población.\n","    En ese caso el accuracy del modelo si se aprende esos datos, para esos modelos será alto para esa predecir la categoría \"estrella\", y no tan bueno para las otras (modelo sesgado). Si le doy datos nuevos el desempeño no será óptimo. \n","    La presición será alta para la categoría estrella no así para las demás. Lo contrario ocurre con el recall, sería bajo dado que con nuevos datos puedo tener más falsos negativos (sorpresas) por el sesgo de mi modelo.\n","\n","\n","**cuando te interesa la clase minoritaria. ¿Qué ocurrirá con la Curva ROC y la AUC-ROC?**\n","\n","con la curva de roc para la clase minoritaria tanto para los train y test el resultado tiende a ser menor que el de la mayoritaria porque esta sesgado hacia esta última.\n","\n","\n","El area bajo la curva es menor a 1, ya que el modelo no va a predecir bien nuevos datos ya que está sesgado.\n","tiende a 0,5 que es cuando el modelo no tiene capacidad de discriminación en FP y FN, no estan bien separadas las clases\n","https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc?hl=es-419\n","\n","\n","\n","\n","Para combatir datos desbalanceados\n","https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n","\n","Formas de  foldear y depende de los tipos de datos \n","https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html#sphx-glr-auto-examples-model-selection-plot-cv-indices-py\n","\n","(aveces pueden caer en undersampling u oversampling. ver clase)"]},{"cell_type":"code","metadata":{"id":"L6RwOZzl7lJG"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yfi205IIFivC"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J3r-cU4y7uYJ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O5wOSYnJ7utt"},"source":["#Bitacora 22\n","\n","📝 Elige un modelo que hayamos visto y decide cuáles son sus parámetros e hiperparámetros.\n","\n","\n","KNN no tiene parámetros, hiperparametro k-vecinos,metrica de distancia, peso entre vecinos.\n","DT el parametro de división de categorías, hiperparametro profundidad del arbol, cantidad de hojas, formas de división(ganancia de gini, entropia).\n","\n","\n","📝 Mira [este video](https://www.youtube.com/watch?v=HdlDYng8g9s) (está en inglés, ¡pero vale la pena!) que además viene con [Notebook incluída](https://github.com/codebasics/py/blob/master/ML/15_gridsearch/15_grid_search.ipynb).\n","\n","\n","\n","\n","Ejercicio - Challenge: en bitacora"]},{"cell_type":"code","metadata":{"id":"UV-7O4Mq7wRn"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f4utwfDtQyyi"},"source":["\n","**Ejercicio - Challenge:** Repite lo que hicimos, pero para un árbol de decisión. Algunos hiperparámetros que pueden ser interesantes de explorar, en este caso, son: `criterion`, `max_depth`, `min_samples_split` y `min_samples_leaf`."]},{"cell_type":"code","metadata":{"id":"2CR9807lRsmj"},"source":["X = data.drop(['target'],axis=1)\n","y = data['target']\n","\n","# Dividimos los datos en Train y Test\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pt5rb59GQMFa","colab":{"base_uri":"https://localhost:8080/","height":56},"executionInfo":{"status":"ok","timestamp":1603672129138,"user_tz":180,"elapsed":720,"user":{"displayName":"Martin Iglesias","photoUrl":"https://lh5.googleusercontent.com/-8Fjp34ixMNs/AAAAAAAAAAI/AAAAAAAAAdQ/B0KVwFNI_pc/s64/photo.jpg","userId":"09692603429409562435"}},"outputId":"454c5fbb-d1d9-4d37-e762-31ddaa3e2759"},"source":["#DT classifier\n","from sklearn.tree import DecisionTreeClassifier\n","dtree_model=DecisionTreeClassifier()\n","#veo los parametros \n","dtree_model.get_params().keys()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['ccp_alpha', 'class_weight', 'criterion', 'max_depth', 'max_features', 'max_leaf_nodes', 'min_impurity_decrease', 'min_impurity_split', 'min_samples_leaf', 'min_samples_split', 'min_weight_fraction_leaf', 'presort', 'random_state', 'splitter'])"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"GCyOWp3MRxRx"},"source":["# Grilla para Grid Search\n","#https://stackoverflow.com/questions/38709690/scikit-learn-using-gridsearchcv-on-decisiontreeclassifier\n","#criterion, max_depth, min_samples_split y min_samples_leaf.\n","max_depth_range=np.arange(2, 20)\n","min_samples_split_range=np.arange(2, 10)\n","min_samples_leaf_range=np.arange(2, 10)\n","tree_param = [{'criterion': ['entropy', 'gini'], 'max_depth': max_depth_range},\n","              {'min_samples_leaf': min_samples_leaf_range,'min_samples_split': min_samples_split_range}]\n","\n","\n","#seguir\n","#https://medium.com/analytics-vidhya/decisiontree-classifier-working-on-moons-dataset-using-gridsearchcv-to-find-best-hyperparameters-ede24a06b489\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WEIIvgVURxSQ"},"source":["Una vez definida la grilla, ya podemos entrenar el modelo. "]},{"cell_type":"code","metadata":{"id":"VDPKVHM1RxSS","colab":{"base_uri":"https://localhost:8080/","height":410},"executionInfo":{"status":"ok","timestamp":1603672231034,"user_tz":180,"elapsed":3010,"user":{"displayName":"Martin Iglesias","photoUrl":"https://lh5.googleusercontent.com/-8Fjp34ixMNs/AAAAAAAAAAI/AAAAAAAAAdQ/B0KVwFNI_pc/s64/photo.jpg","userId":"09692603429409562435"}},"outputId":"1e039fc3-888e-47a6-d338-e8db0a1054d0"},"source":["# ESTRATEGIA 1: Grid Search\n","model = GridSearchCV(dtree_model, param_grid=tree_param, cv=5)\n","# Entrenamos: KNN con la grilla definida arriba y CV con tamaño de Fold=5\n","model.fit(X_train, y_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GridSearchCV(cv=5, error_score=nan,\n","             estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,\n","                                              criterion='gini', max_depth=None,\n","                                              max_features=None,\n","                                              max_leaf_nodes=None,\n","                                              min_impurity_decrease=0.0,\n","                                              min_impurity_split=None,\n","                                              min_samples_leaf=1,\n","                                              min_samples_split=2,\n","                                              min_weight_fraction_leaf=0.0,\n","                                              presort='deprecated',\n","                                              random_state=None,\n","                                              splitter='best'),\n","             iid='deprecated', n_jobs=None,\n","             param_grid=[{'criterion': ['entropy', 'gini'],\n","                          'max_depth': array([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n","       19])},\n","                         {'min_samples_leaf': array([2, 3, 4, 5, 6, 7, 8, 9]),\n","                          'min_samples_split': array([2, 3, 4, 5, 6, 7, 8, 9])}],\n","             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n","             scoring=None, verbose=0)"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"Y6hlDI6gUNIm","colab":{"base_uri":"https://localhost:8080/","height":847},"executionInfo":{"status":"ok","timestamp":1603672321544,"user_tz":180,"elapsed":779,"user":{"displayName":"Martin Iglesias","photoUrl":"https://lh5.googleusercontent.com/-8Fjp34ixMNs/AAAAAAAAAAI/AAAAAAAAAdQ/B0KVwFNI_pc/s64/photo.jpg","userId":"09692603429409562435"}},"outputId":"4fbdefcd-c710-4597-c676-1b93e549a439"},"source":["print(\"Mejores parametros: \"+str(model.best_params_))\n","print(\"Mejor Score: \"+str(model.best_score_)+'\\n')\n","\n","scores = pd.DataFrame(model.cv_results_)\n","scores\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mejores parametros: {'criterion': 'gini', 'max_depth': 3}\n","Mejor Score: 0.9341997264021888\n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>mean_fit_time</th>\n","      <th>std_fit_time</th>\n","      <th>mean_score_time</th>\n","      <th>std_score_time</th>\n","      <th>param_criterion</th>\n","      <th>param_max_depth</th>\n","      <th>param_min_samples_leaf</th>\n","      <th>param_min_samples_split</th>\n","      <th>params</th>\n","      <th>split0_test_score</th>\n","      <th>split1_test_score</th>\n","      <th>split2_test_score</th>\n","      <th>split3_test_score</th>\n","      <th>split4_test_score</th>\n","      <th>mean_test_score</th>\n","      <th>std_test_score</th>\n","      <th>rank_test_score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.003477</td>\n","      <td>0.001202</td>\n","      <td>0.000967</td>\n","      <td>0.000048</td>\n","      <td>entropy</td>\n","      <td>2</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>{'criterion': 'entropy', 'max_depth': 2}</td>\n","      <td>0.918605</td>\n","      <td>0.929412</td>\n","      <td>0.917647</td>\n","      <td>0.882353</td>\n","      <td>0.929412</td>\n","      <td>0.915486</td>\n","      <td>0.017321</td>\n","      <td>23</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.003306</td>\n","      <td>0.000221</td>\n","      <td>0.000974</td>\n","      <td>0.000080</td>\n","      <td>entropy</td>\n","      <td>3</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>{'criterion': 'entropy', 'max_depth': 3}</td>\n","      <td>0.918605</td>\n","      <td>0.929412</td>\n","      <td>0.905882</td>\n","      <td>0.917647</td>\n","      <td>0.894118</td>\n","      <td>0.913133</td>\n","      <td>0.012079</td>\n","      <td>29</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.003989</td>\n","      <td>0.000713</td>\n","      <td>0.000983</td>\n","      <td>0.000135</td>\n","      <td>entropy</td>\n","      <td>4</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>{'criterion': 'entropy', 'max_depth': 4}</td>\n","      <td>0.965116</td>\n","      <td>0.929412</td>\n","      <td>0.905882</td>\n","      <td>0.905882</td>\n","      <td>0.952941</td>\n","      <td>0.931847</td>\n","      <td>0.024108</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.003741</td>\n","      <td>0.000135</td>\n","      <td>0.001076</td>\n","      <td>0.000038</td>\n","      <td>entropy</td>\n","      <td>5</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>{'criterion': 'entropy', 'max_depth': 5}</td>\n","      <td>0.953488</td>\n","      <td>0.917647</td>\n","      <td>0.894118</td>\n","      <td>0.894118</td>\n","      <td>0.929412</td>\n","      <td>0.917756</td>\n","      <td>0.022495</td>\n","      <td>18</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.003854</td>\n","      <td>0.000182</td>\n","      <td>0.001094</td>\n","      <td>0.000178</td>\n","      <td>entropy</td>\n","      <td>6</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>{'criterion': 'entropy', 'max_depth': 6}</td>\n","      <td>0.941860</td>\n","      <td>0.917647</td>\n","      <td>0.882353</td>\n","      <td>0.894118</td>\n","      <td>0.917647</td>\n","      <td>0.910725</td>\n","      <td>0.020717</td>\n","      <td>50</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>95</th>\n","      <td>0.002688</td>\n","      <td>0.000077</td>\n","      <td>0.000895</td>\n","      <td>0.000021</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>9</td>\n","      <td>5</td>\n","      <td>{'min_samples_leaf': 9, 'min_samples_split': 5}</td>\n","      <td>0.895349</td>\n","      <td>0.917647</td>\n","      <td>0.917647</td>\n","      <td>0.894118</td>\n","      <td>0.917647</td>\n","      <td>0.908482</td>\n","      <td>0.011232</td>\n","      <td>55</td>\n","    </tr>\n","    <tr>\n","      <th>96</th>\n","      <td>0.002676</td>\n","      <td>0.000064</td>\n","      <td>0.000901</td>\n","      <td>0.000012</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>9</td>\n","      <td>6</td>\n","      <td>{'min_samples_leaf': 9, 'min_samples_split': 6}</td>\n","      <td>0.895349</td>\n","      <td>0.905882</td>\n","      <td>0.917647</td>\n","      <td>0.894118</td>\n","      <td>0.917647</td>\n","      <td>0.906129</td>\n","      <td>0.010256</td>\n","      <td>72</td>\n","    </tr>\n","    <tr>\n","      <th>97</th>\n","      <td>0.002735</td>\n","      <td>0.000035</td>\n","      <td>0.000912</td>\n","      <td>0.000019</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>9</td>\n","      <td>7</td>\n","      <td>{'min_samples_leaf': 9, 'min_samples_split': 7}</td>\n","      <td>0.895349</td>\n","      <td>0.905882</td>\n","      <td>0.917647</td>\n","      <td>0.894118</td>\n","      <td>0.917647</td>\n","      <td>0.906129</td>\n","      <td>0.010256</td>\n","      <td>72</td>\n","    </tr>\n","    <tr>\n","      <th>98</th>\n","      <td>0.002656</td>\n","      <td>0.000038</td>\n","      <td>0.000860</td>\n","      <td>0.000032</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>9</td>\n","      <td>8</td>\n","      <td>{'min_samples_leaf': 9, 'min_samples_split': 8}</td>\n","      <td>0.895349</td>\n","      <td>0.905882</td>\n","      <td>0.917647</td>\n","      <td>0.894118</td>\n","      <td>0.917647</td>\n","      <td>0.906129</td>\n","      <td>0.010256</td>\n","      <td>72</td>\n","    </tr>\n","    <tr>\n","      <th>99</th>\n","      <td>0.002649</td>\n","      <td>0.000040</td>\n","      <td>0.000870</td>\n","      <td>0.000010</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>9</td>\n","      <td>9</td>\n","      <td>{'min_samples_leaf': 9, 'min_samples_split': 9}</td>\n","      <td>0.895349</td>\n","      <td>0.917647</td>\n","      <td>0.917647</td>\n","      <td>0.894118</td>\n","      <td>0.917647</td>\n","      <td>0.908482</td>\n","      <td>0.011232</td>\n","      <td>55</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>100 rows × 17 columns</p>\n","</div>"],"text/plain":["    mean_fit_time  std_fit_time  ...  std_test_score  rank_test_score\n","0        0.003477      0.001202  ...        0.017321               23\n","1        0.003306      0.000221  ...        0.012079               29\n","2        0.003989      0.000713  ...        0.024108                2\n","3        0.003741      0.000135  ...        0.022495               18\n","4        0.003854      0.000182  ...        0.020717               50\n","..            ...           ...  ...             ...              ...\n","95       0.002688      0.000077  ...        0.011232               55\n","96       0.002676      0.000064  ...        0.010256               72\n","97       0.002735      0.000035  ...        0.010256               72\n","98       0.002656      0.000038  ...        0.010256               72\n","99       0.002649      0.000040  ...        0.011232               55\n","\n","[100 rows x 17 columns]"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"id":"AlICozcIUNGb","colab":{"base_uri":"https://localhost:8080/","height":279},"executionInfo":{"status":"ok","timestamp":1603672331137,"user_tz":180,"elapsed":853,"user":{"displayName":"Martin Iglesias","photoUrl":"https://lh5.googleusercontent.com/-8Fjp34ixMNs/AAAAAAAAAAI/AAAAAAAAAdQ/B0KVwFNI_pc/s64/photo.jpg","userId":"09692603429409562435"}},"outputId":"6d47836b-0eed-47fc-aba0-4f599ff81cd0"},"source":["#Predecimos en los datos de test\n","prediction = model.predict(X_test)\n","\n","# Matriz de Confusion\n","cm = confusion_matrix(y_test,prediction)\n","print(\"Matriz de confusión:\")\n","print(cm)\n","print('Exactitud:', accuracy_score(y_test, prediction))\n","\n","# Reporte de Clasificacion\n","report = classification_report(y_test, prediction)\n","print(\"Reporte de Clasificación:\")\n","print(report)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Matriz de confusión:\n","[[52  2]\n"," [ 7 82]]\n","Exactitud: 0.9370629370629371\n","Reporte de Clasificación:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.88      0.96      0.92        54\n","         1.0       0.98      0.92      0.95        89\n","\n","    accuracy                           0.94       143\n","   macro avg       0.93      0.94      0.93       143\n","weighted avg       0.94      0.94      0.94       143\n","\n"],"name":"stdout"}]}]}