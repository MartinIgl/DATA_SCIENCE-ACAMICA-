{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Challenge_B2","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPQbxdY0ZMZDjtv9TPenol3"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"B_JpdfO35CGx"},"source":["# Bitacora 19-20\n","\n","**Ejercicio 1 - Challenge 1:** Comienza con una estrategia sencilla, imputar con el valor medio de cada columna usando Pandas. \n","\n","쯇ara cu치les columnas tendr치 sentido esto, teniendo en cuenta lo visto en el punto anterior? \n","Para aquellas columnas numericas V3, V4 y V5.\n","\n","\n","Por las dudas, comenzamos volviendo a cargar los datos:\n","\n"]},{"cell_type":"code","metadata":{"id":"WaRhi24749iz"},"source":["import numpy as np\n","import pandas as pd \n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","\n","\n","\n","from google.colab import drive\n","drive.mount('/content/drive') #monto el drive en collab y leo el file \"DS_Proyecto_01_Datos_Properati.csv\" insertando la url con extension csv\n","\n","ValorAtipico1=pd.read_csv('/content/drive/My Drive/Colab Notebooks/DS_Bita패cora_19_Data_VA1.csv', sep=',',engine='python')\n","ValorAtipico2=pd.read_csv('/content/drive/My Drive/Colab Notebooks/DS_Bita패cora_19_Data_VA2.csv', sep=',',engine='python')\n","ValorFaltante=pd.read_csv('/content/drive/My Drive/Colab Notebooks/DS_Bita패cora_19_Data_con_VF.csv', sep=',',engine='python')\n","Valor_distrofia=pd.read_csv('/content/drive/My Drive/Colab Notebooks/DS_Bita패cora_19_Distrofia.csv', sep=',',engine='python')\n","\n","\n","dataVF = ValorFaltante.copy()\n","\n","print(dataVF)\n","datasinVF=dataVF.dropna()\n","print(datasinVF)\n","\n","#forma intuitiva es llenar con la media\n","dataVF1 = ValorFaltante.copy()\n","dataVF1.fillna(dataVF1.mean(),inplace=True) # es independiente del lugar que veo. supongo que cualquier valor faltante reemplace con la media (vale si la desviacion es peque침a y una distribucion normal es bueno, porque los datos son muy parecidos) si tiene curtosis alta, muy aplanada no es bueno\n","\n","#histogramas de las variables con y sin valores faltantes.\n","dataVF[[\"V2\", \"V3\",'V4','V5']].plot.hist(alpha=0.4, bins=100) \n","datasinVF[[\"V2\", \"V3\",'V4','V5']].plot.hist(alpha=0.4, bins=100) \n","\n","sns.pairplot(dataVF1,hue='V1', hue_order=['A','B','C']) \n","# como impute los valores faltantes por la media, veo la distribuin m치s 췂picuda\" entonces"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zObdk-1-7iLn"},"source":["#inputo los valores faltantes con la media de cada columna\n","print((dataVF.isna()).sum())\n","\n","#---- Selective Treatment----------------\n","for i in dataVF.columns[dataVF.isnull().any(axis=0)]:     #---Applying Only on variables with NaN values\n","    dataVF[i].fillna(dataVF[i].mean(),inplace=True)\n","\n","print((dataVF.isna()).sum())\n","\n","plt.figure()\n","Dat=dataVF.isna()\n","heat=sns.heatmap(Dat.T,  cmap=\"YlGnBu\",xticklabels=False)\n","plt.title('Datos Faltantes (=1) dentro del Dataset')\n","plt.xlabel('Instancias dentro del Dataset')\n","plt.ylabel('Elementos columna del Dataset')\n","plt.tight_layout()\n","plt.show()\n","\n","#Al imputar, 쯖ambi치n las distribuciones?쮼sto est치 bien o mal?\n","dataVF[[\"V2\", \"V3\",'V4','V5']].plot.hist(alpha=0.4, bins=100) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sj47fG2N-LqT"},"source":["## 3. Escalado de datos - Challenge 2\n","\n","Seguiremos usando los datos de la secci칩n anterior:\n","\n","1. Agregar al dataframe una columna que se llame \"Datos_Reescalados\" y que contenga los datos reescalados por Z-Score. 쮺u치l es el valor medio de los datos reescalados?쯏 su desviaci칩n est치ndar?\n","\n"," **Nota:** Para reescalar los datos, pueden hacerlo \"a mano\" o utilizar la clase `StandardScaler` de Scikit-Learn. No te olvides que las herramientas de preprocesamiento de datos de Scikit-learn tienen los mismos m칠todos (crear el objeto, fitearlo, transfromar los datos). Si necesit치s ayuda, pod칠s consultar la [documentaci칩n](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n","2. Realiz치, en un mismo gr치fico, un histograma de los datos reescaleados y un histograma de los datos crudos. 쯈u칠 similitudes y qu칠 diferencias tiene con la distribuci칩n de datos crudos?\n","3. Aplica la regla de las tres sigmas utilizando los datos reescalados."]},{"cell_type":"code","metadata":{"id":"2YpHonTX-MmN"},"source":["#1. Agregar al dataframe una columna que se llame \"Datos_Reescalados\" y que contenga los datos reescalados por Z-Score.\n","# 쮺u치l es el valor medio de los datos reescalados?쯏 su desviaci칩n est치ndar?\n","#los datos reescalados idealmente tienen una media poblacional cero y un desvio de 1.\n","\n","dataVa1=ValorAtipico1.copy()\n","dataVa2=ValorAtipico2.copy()\n","\n","#reescalo con la normalizaci칩n(estandarizo el valor x-mu/sigma) a mano\n","valor_medio = dataVa1.mean()  #dataVa1.Datos.mean()\n","std = dataVa1.std() #dataVa1.Datos.std()\n","print('el valor medio de mis datos es {} y la desviacion estandar {}'.format(float(valor_medio),float(std)))\n","dataVa1['Datos_Reescalados'] = (dataVa1-valor_medio)/std\n","print(dataVa1.head())\n","print('la media del dato estandarizado {} y el desvio {}'.format(dataVa1['Datos_Reescalados'].mean(),dataVa1['Datos_Reescalados'].std()))\n","\n","print()\n","#reescalo con la normalizaci칩n(estandarizo el valor x-mu/sigma)\n","valor_medio = dataVa2.mean()\n","std = dataVa2.std()\n","print('el valor medio {} y la desviacion estandar {}'.format(float(valor_medio),float(std)))\n","dataVa2['Datos_Reescalados'] = (dataVa2-valor_medio)/std\n","print(dataVa2.head())\n","print('la media del dato estandarizado {} y el desvio {}'.format(dataVa2['Datos_Reescalados'].mean(),dataVa2['Datos_Reescalados'].std()))\n","#cada dato esta +-X alejado en desvios estandar respecto de la media. es decir si el valor es -1,28 esta a -1,28 desivios estandar respecto de la media (el desvio es 1 aca).\n","\n","\n","#normalized_df=(df-df.mean())/df.std()\n","#normalized_df=(df-df.min())/(df.max()-df.min()\n","\n","\n","\n","#data.Datos.hist(label='Datos crudos')\n","#data.Datos_Reescalados.hist(label='reescalados)\n","#plt.legend\n","#plt.show()\n","\n","#regla de 3 sigma\n","#mascara_outlier=np.logical_or(Datos_reescaldos<-3,Datos_reescaldos>3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I2cD1fQK-P7t"},"source":["#lo hago con sklearn\n","#Nota: Para reescalar los datos, pueden hacerlo \"a mano\" o utilizar la clase StandardScaler de Scikit-Learn. \n","#No te olvides que las herramientas de preprocesamiento de datos de Scikit-learn tienen los mismos m칠todos (crear el objeto, fitearlo, transfromar los datos). \n","#https://scikit-learn.org/stable/auto_examples/release_highlights/plot_release_highlights_0_23_0.html#sphx-glr-auto-examples-release-highlights-plot-release-highlights-0-23-0-py\n","#https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n","\n","import pandas as pd\n","from sklearn import preprocessing\n","dataVa1=ValorAtipico1.copy()\n","dataVa2=ValorAtipico2.copy()\n","\n","x = dataVa1.values #returns a numpy array\n","STscaler = preprocessing.StandardScaler()\n","x_STscaled = STscaler.fit_transform(x)\n","dataVa1['Datos_Reescalados_estandarizado'] =pd.DataFrame(x_STscaled)\n","\n","min_max_scaler = preprocessing.MinMaxScaler()\n","x_scaledminmax = min_max_scaler.fit_transform(x)\n","dataVa1['Datos_Reescalados_minmax'] =pd.DataFrame(x_scaledminmax)\n","\n","print(dataVa1)\n","\n","x = dataVa2.values #returns a numpy array\n","STscaler = preprocessing.StandardScaler()\n","x_STscaled = STscaler.fit_transform(x)\n","dataVa2['Datos_Reescalados_estandarizado'] =pd.DataFrame(x_STscaled)\n","\n","min_max_scaler = preprocessing.MinMaxScaler()\n","x_scaledminmax = min_max_scaler.fit_transform(x)\n","dataVa2['Datos_Reescalados_minmax'] =pd.DataFrame(x_scaledminmax)\n","\n","print(dataVa2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TgbzfO3v-R7x"},"source":["#Realiz치, en un mismo gr치fico, un histograma de los datos reescaleados y un histograma de los datos crudos. \n","#쯈u칠 similitudes y qu칠 diferencias tiene con la distribuci칩n de datos crudos?\n","plt.figure()\n","sns.distplot(dataVa1['Datos'])\n","sns.distplot(dataVa1['Datos_Reescalados_estandarizado'])\n","\n","plt.figure()\n","sns.distplot(dataVa2['Datos'])\n","sns.distplot(dataVa2['Datos_Reescalados_estandarizado'])\n","\n","#se modifica la curtosis, la forma de la distribuci칩n es casisimilar. hay m치s datos en el centro."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0A4ip7U--UCn"},"source":["#Aplica la regla de las tres sigmas utilizando los datos reescalados.\n","#la regla de las tres sigmas \n","def OutlierSD(X,Atributo):\n","     \"\"\"\n","      Se considera que los valores Outliers se obtienen a partir de los valores:\n","      Por encima de media + 3 * SD y por debajo de media - 3 * SD.\n","      Ante eso se buscan los valores que son outliers inferior y superior de mi dataset en algun atributo en particular.\n","      1ro se calculan el promedio y la desviacion estandar.\n","      2do se calculan los valores para delimitar los extremos.\n","     \"\"\"\n","     #Calculo los limites para cada tipo de propiedad principal para contar y filtrar los outliers\n","     mean=X[Atributo].mean()\n","     S=X[Atributo].std()\n","     L_s_inf= mean-3*S\n","     L_s_sup= mean+3*S\n","     return float(L_s_inf), float(L_s_sup)\n","\n","\n","L_inf_s_a1 ,L_sup_s_a1= OutlierSD(dataVa1,\"Datos_Reescalados_estandarizado\")\n","L_inf_s_a2 ,L_sup_s_a2= OutlierSD(dataVa2,\"Datos_Reescalados_estandarizado\")\n","print('Los valores de los l칤mites extremos (L_inf,L_sup) de V1: '+str((L_inf_s_a1,L_sup_s_a1))+' y V2: ' +str((L_inf_s_a2,L_sup_s_a2)))\n","\n","print('La cantidad de outliers sup de V1: '+str((dataVa1.Datos_Reescalados_estandarizado>L_sup_s_a1).sum()) +' y V2: ',str((dataVa2.Datos_Reescalados_estandarizado>L_sup_s_a2).sum()))\n","print('La cantidad de outliers inf de V1: '+str((dataVa1.Datos_Reescalados_estandarizado<L_inf_s_a1).sum()) +' y V2: ',str((dataVa2.Datos_Reescalados_estandarizado<L_inf_s_a2).sum()))\n","\n","sns.boxplot(x=dataVa1[\"Datos_Reescalados_estandarizado\"],notch=True, whis=3)\n","sns.boxplot(x=dataVa2[\"Datos_Reescalados_estandarizado\"],notch=True, whis=3)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cxPHAuoc-WOJ"},"source":["\n","ValorAtipico1_woOut_SD=dataVa1[dataVa1.Datos_Reescalados_estandarizado < L_sup_s_a1]\n","ValorAtipico1_woOut_SD=ValorAtipico1_woOut_SD[ValorAtipico1_woOut_SD.Datos_Reescalados_estandarizado > L_inf_s_a1]\n","\n","ValorAtipico2_woOut_SD=dataVa2[dataVa2.Datos_Reescalados_estandarizado < L_sup_s_a2]\n","ValorAtipico2_woOut_SD=ValorAtipico2_woOut_SD[ValorAtipico2_woOut_SD.Datos_Reescalados_estandarizado > L_inf_s_a2]\n","\n","\n","plt.figure()\n","sns.distplot(dataVa1['Datos'])\n","sns.distplot(dataVa1['Datos_Reescalados_estandarizado'])\n","sns.distplot(ValorAtipico1_woOut_SD['Datos_Reescalados_estandarizado'])\n","plt.figure()\n","sns.distplot(dataVa2['Datos'])\n","sns.distplot(dataVa2['Datos_Reescalados_estandarizado'])\n","sns.distplot(ValorAtipico2_woOut_SD['Datos_Reescalados_estandarizado'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xe0_075M-aJj"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"51Wz0Py8-aFS"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UqhMoXQ07ino"},"source":["#Encuentro 21 challenge\n","\n","Para pensar e investigar. Existen dataset desbalanceados, es decir, conjuntos de datos donde hay clases sobrerrepresentadas y otras subrepresentadas. Por ejemplo, el dataset de Iris es un dataset perfectamente balanceado (50 instancias de cada clase), mientras que el dataset del Titanic est치 levemente desbalanceado. Trabajar con datasets muy desbalanceados no es tarea sencilla, y existen muchas t칠cnicas para abordarlos. \n","\n","Reflexiona sobre qu칠 ocurre con la exactitud, la precisi칩n y la exhaustividad cuando las utilizas para evaluar desempe침o sobre dataset binario muy desbalanceado (supongamos, proporci칩n 1 en 1000)\n","\n","    ->Si los datos est치n desbalanceado quiere decir que el conjunto de datos no es homog칠neo y tiende a reinar una categor칤a por sobre otra, por lo que la muestra puede estar sesgada respecto a la poblaci칩n.\n","    En ese caso el accuracy del modelo si se aprende esos datos, para esos modelos ser치 alto para esa predecir la categor칤a \"estrella\", y no tan bueno para las otras (modelo sesgado). Si le doy datos nuevos el desempe침o no ser치 칩ptimo. \n","    La presici칩n ser치 alta para la categor칤a estrella no as칤 para las dem치s. Lo contrario ocurre con el recall, ser칤a bajo dado que con nuevos datos puedo tener m치s falsos negativos (sorpresas) por el sesgo de mi modelo.\n","\n","\n","**cuando te interesa la clase minoritaria. 쯈u칠 ocurrir치 con la Curva ROC y la AUC-ROC?**\n","\n","con la curva de roc para la clase minoritaria tanto para los train y test el resultado tiende a ser menor que el de la mayoritaria porque esta sesgado hacia esta 칰ltima.\n","\n","\n","El area bajo la curva es menor a 1, ya que el modelo no va a predecir bien nuevos datos ya que est치 sesgado.\n","tiende a 0,5 que es cuando el modelo no tiene capacidad de discriminaci칩n en FP y FN, no estan bien separadas las clases\n","https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc?hl=es-419\n","\n","\n","\n","\n","Para combatir datos desbalanceados\n","https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n","\n","Formas de  foldear y depende de los tipos de datos \n","https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html#sphx-glr-auto-examples-model-selection-plot-cv-indices-py\n","\n","(aveces pueden caer en undersampling u oversampling. ver clase)"]},{"cell_type":"code","metadata":{"id":"L6RwOZzl7lJG"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yfi205IIFivC"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J3r-cU4y7uYJ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O5wOSYnJ7utt"},"source":["#Bitacora 22\n","\n","游닇 Elige un modelo que hayamos visto y decide cu치les son sus par치metros e hiperpar치metros.\n","\n","\n","KNN no tiene par치metros, hiperparametro k-vecinos,metrica de distancia, peso entre vecinos.\n","DT el parametro de divisi칩n de categor칤as, hiperparametro profundidad del arbol, cantidad de hojas, formas de divisi칩n(ganancia de gini, entropia).\n","\n","\n","游닇 Mira [este video](https://www.youtube.com/watch?v=HdlDYng8g9s) (est치 en ingl칠s, 춰pero vale la pena!) que adem치s viene con [Notebook inclu칤da](https://github.com/codebasics/py/blob/master/ML/15_gridsearch/15_grid_search.ipynb).\n","\n","\n","\n","\n","Ejercicio - Challenge: en bitacora"]},{"cell_type":"code","metadata":{"id":"UV-7O4Mq7wRn"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f4utwfDtQyyi"},"source":["\n","**Ejercicio - Challenge:** Repite lo que hicimos, pero para un 치rbol de decisi칩n. Algunos hiperpar치metros que pueden ser interesantes de explorar, en este caso, son: `criterion`, `max_depth`, `min_samples_split` y `min_samples_leaf`."]},{"cell_type":"code","metadata":{"id":"2CR9807lRsmj"},"source":["X = data.drop(['target'],axis=1)\n","y = data['target']\n","\n","# Dividimos los datos en Train y Test\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pt5rb59GQMFa","colab":{"base_uri":"https://localhost:8080/","height":56},"executionInfo":{"status":"ok","timestamp":1603672129138,"user_tz":180,"elapsed":720,"user":{"displayName":"Martin Iglesias","photoUrl":"https://lh5.googleusercontent.com/-8Fjp34ixMNs/AAAAAAAAAAI/AAAAAAAAAdQ/B0KVwFNI_pc/s64/photo.jpg","userId":"09692603429409562435"}},"outputId":"454c5fbb-d1d9-4d37-e762-31ddaa3e2759"},"source":["#DT classifier\n","from sklearn.tree import DecisionTreeClassifier\n","dtree_model=DecisionTreeClassifier()\n","#veo los parametros \n","dtree_model.get_params().keys()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['ccp_alpha', 'class_weight', 'criterion', 'max_depth', 'max_features', 'max_leaf_nodes', 'min_impurity_decrease', 'min_impurity_split', 'min_samples_leaf', 'min_samples_split', 'min_weight_fraction_leaf', 'presort', 'random_state', 'splitter'])"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"GCyOWp3MRxRx"},"source":["# Grilla para Grid Search\n","#https://stackoverflow.com/questions/38709690/scikit-learn-using-gridsearchcv-on-decisiontreeclassifier\n","#criterion, max_depth, min_samples_split y min_samples_leaf.\n","max_depth_range=np.arange(2, 20)\n","min_samples_split_range=np.arange(2, 10)\n","min_samples_leaf_range=np.arange(2, 10)\n","tree_param = [{'criterion': ['entropy', 'gini'], 'max_depth': max_depth_range},\n","              {'min_samples_leaf': min_samples_leaf_range,'min_samples_split': min_samples_split_range}]\n","\n","\n","#seguir\n","#https://medium.com/analytics-vidhya/decisiontree-classifier-working-on-moons-dataset-using-gridsearchcv-to-find-best-hyperparameters-ede24a06b489\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WEIIvgVURxSQ"},"source":["Una vez definida la grilla, ya podemos entrenar el modelo. "]},{"cell_type":"code","metadata":{"id":"VDPKVHM1RxSS","colab":{"base_uri":"https://localhost:8080/","height":410},"executionInfo":{"status":"ok","timestamp":1603672231034,"user_tz":180,"elapsed":3010,"user":{"displayName":"Martin Iglesias","photoUrl":"https://lh5.googleusercontent.com/-8Fjp34ixMNs/AAAAAAAAAAI/AAAAAAAAAdQ/B0KVwFNI_pc/s64/photo.jpg","userId":"09692603429409562435"}},"outputId":"1e039fc3-888e-47a6-d338-e8db0a1054d0"},"source":["# ESTRATEGIA 1: Grid Search\n","model = GridSearchCV(dtree_model, param_grid=tree_param, cv=5)\n","# Entrenamos: KNN con la grilla definida arriba y CV con tama침o de Fold=5\n","model.fit(X_train, y_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GridSearchCV(cv=5, error_score=nan,\n","             estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,\n","                                              criterion='gini', max_depth=None,\n","                                              max_features=None,\n","                                              max_leaf_nodes=None,\n","                                              min_impurity_decrease=0.0,\n","                                              min_impurity_split=None,\n","                                              min_samples_leaf=1,\n","                                              min_samples_split=2,\n","                                              min_weight_fraction_leaf=0.0,\n","                                              presort='deprecated',\n","                                              random_state=None,\n","                                              splitter='best'),\n","             iid='deprecated', n_jobs=None,\n","             param_grid=[{'criterion': ['entropy', 'gini'],\n","                          'max_depth': array([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n","       19])},\n","                         {'min_samples_leaf': array([2, 3, 4, 5, 6, 7, 8, 9]),\n","                          'min_samples_split': array([2, 3, 4, 5, 6, 7, 8, 9])}],\n","             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n","             scoring=None, verbose=0)"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"Y6hlDI6gUNIm","colab":{"base_uri":"https://localhost:8080/","height":847},"executionInfo":{"status":"ok","timestamp":1603672321544,"user_tz":180,"elapsed":779,"user":{"displayName":"Martin Iglesias","photoUrl":"https://lh5.googleusercontent.com/-8Fjp34ixMNs/AAAAAAAAAAI/AAAAAAAAAdQ/B0KVwFNI_pc/s64/photo.jpg","userId":"09692603429409562435"}},"outputId":"4fbdefcd-c710-4597-c676-1b93e549a439"},"source":["print(\"Mejores parametros: \"+str(model.best_params_))\n","print(\"Mejor Score: \"+str(model.best_score_)+'\\n')\n","\n","scores = pd.DataFrame(model.cv_results_)\n","scores\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mejores parametros: {'criterion': 'gini', 'max_depth': 3}\n","Mejor Score: 0.9341997264021888\n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>mean_fit_time</th>\n","      <th>std_fit_time</th>\n","      <th>mean_score_time</th>\n","      <th>std_score_time</th>\n","      <th>param_criterion</th>\n","      <th>param_max_depth</th>\n","      <th>param_min_samples_leaf</th>\n","      <th>param_min_samples_split</th>\n","      <th>params</th>\n","      <th>split0_test_score</th>\n","      <th>split1_test_score</th>\n","      <th>split2_test_score</th>\n","      <th>split3_test_score</th>\n","      <th>split4_test_score</th>\n","      <th>mean_test_score</th>\n","      <th>std_test_score</th>\n","      <th>rank_test_score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.003477</td>\n","      <td>0.001202</td>\n","      <td>0.000967</td>\n","      <td>0.000048</td>\n","      <td>entropy</td>\n","      <td>2</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>{'criterion': 'entropy', 'max_depth': 2}</td>\n","      <td>0.918605</td>\n","      <td>0.929412</td>\n","      <td>0.917647</td>\n","      <td>0.882353</td>\n","      <td>0.929412</td>\n","      <td>0.915486</td>\n","      <td>0.017321</td>\n","      <td>23</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.003306</td>\n","      <td>0.000221</td>\n","      <td>0.000974</td>\n","      <td>0.000080</td>\n","      <td>entropy</td>\n","      <td>3</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>{'criterion': 'entropy', 'max_depth': 3}</td>\n","      <td>0.918605</td>\n","      <td>0.929412</td>\n","      <td>0.905882</td>\n","      <td>0.917647</td>\n","      <td>0.894118</td>\n","      <td>0.913133</td>\n","      <td>0.012079</td>\n","      <td>29</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.003989</td>\n","      <td>0.000713</td>\n","      <td>0.000983</td>\n","      <td>0.000135</td>\n","      <td>entropy</td>\n","      <td>4</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>{'criterion': 'entropy', 'max_depth': 4}</td>\n","      <td>0.965116</td>\n","      <td>0.929412</td>\n","      <td>0.905882</td>\n","      <td>0.905882</td>\n","      <td>0.952941</td>\n","      <td>0.931847</td>\n","      <td>0.024108</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.003741</td>\n","      <td>0.000135</td>\n","      <td>0.001076</td>\n","      <td>0.000038</td>\n","      <td>entropy</td>\n","      <td>5</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>{'criterion': 'entropy', 'max_depth': 5}</td>\n","      <td>0.953488</td>\n","      <td>0.917647</td>\n","      <td>0.894118</td>\n","      <td>0.894118</td>\n","      <td>0.929412</td>\n","      <td>0.917756</td>\n","      <td>0.022495</td>\n","      <td>18</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.003854</td>\n","      <td>0.000182</td>\n","      <td>0.001094</td>\n","      <td>0.000178</td>\n","      <td>entropy</td>\n","      <td>6</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>{'criterion': 'entropy', 'max_depth': 6}</td>\n","      <td>0.941860</td>\n","      <td>0.917647</td>\n","      <td>0.882353</td>\n","      <td>0.894118</td>\n","      <td>0.917647</td>\n","      <td>0.910725</td>\n","      <td>0.020717</td>\n","      <td>50</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>95</th>\n","      <td>0.002688</td>\n","      <td>0.000077</td>\n","      <td>0.000895</td>\n","      <td>0.000021</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>9</td>\n","      <td>5</td>\n","      <td>{'min_samples_leaf': 9, 'min_samples_split': 5}</td>\n","      <td>0.895349</td>\n","      <td>0.917647</td>\n","      <td>0.917647</td>\n","      <td>0.894118</td>\n","      <td>0.917647</td>\n","      <td>0.908482</td>\n","      <td>0.011232</td>\n","      <td>55</td>\n","    </tr>\n","    <tr>\n","      <th>96</th>\n","      <td>0.002676</td>\n","      <td>0.000064</td>\n","      <td>0.000901</td>\n","      <td>0.000012</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>9</td>\n","      <td>6</td>\n","      <td>{'min_samples_leaf': 9, 'min_samples_split': 6}</td>\n","      <td>0.895349</td>\n","      <td>0.905882</td>\n","      <td>0.917647</td>\n","      <td>0.894118</td>\n","      <td>0.917647</td>\n","      <td>0.906129</td>\n","      <td>0.010256</td>\n","      <td>72</td>\n","    </tr>\n","    <tr>\n","      <th>97</th>\n","      <td>0.002735</td>\n","      <td>0.000035</td>\n","      <td>0.000912</td>\n","      <td>0.000019</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>9</td>\n","      <td>7</td>\n","      <td>{'min_samples_leaf': 9, 'min_samples_split': 7}</td>\n","      <td>0.895349</td>\n","      <td>0.905882</td>\n","      <td>0.917647</td>\n","      <td>0.894118</td>\n","      <td>0.917647</td>\n","      <td>0.906129</td>\n","      <td>0.010256</td>\n","      <td>72</td>\n","    </tr>\n","    <tr>\n","      <th>98</th>\n","      <td>0.002656</td>\n","      <td>0.000038</td>\n","      <td>0.000860</td>\n","      <td>0.000032</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>9</td>\n","      <td>8</td>\n","      <td>{'min_samples_leaf': 9, 'min_samples_split': 8}</td>\n","      <td>0.895349</td>\n","      <td>0.905882</td>\n","      <td>0.917647</td>\n","      <td>0.894118</td>\n","      <td>0.917647</td>\n","      <td>0.906129</td>\n","      <td>0.010256</td>\n","      <td>72</td>\n","    </tr>\n","    <tr>\n","      <th>99</th>\n","      <td>0.002649</td>\n","      <td>0.000040</td>\n","      <td>0.000870</td>\n","      <td>0.000010</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>9</td>\n","      <td>9</td>\n","      <td>{'min_samples_leaf': 9, 'min_samples_split': 9}</td>\n","      <td>0.895349</td>\n","      <td>0.917647</td>\n","      <td>0.917647</td>\n","      <td>0.894118</td>\n","      <td>0.917647</td>\n","      <td>0.908482</td>\n","      <td>0.011232</td>\n","      <td>55</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>100 rows 칑 17 columns</p>\n","</div>"],"text/plain":["    mean_fit_time  std_fit_time  ...  std_test_score  rank_test_score\n","0        0.003477      0.001202  ...        0.017321               23\n","1        0.003306      0.000221  ...        0.012079               29\n","2        0.003989      0.000713  ...        0.024108                2\n","3        0.003741      0.000135  ...        0.022495               18\n","4        0.003854      0.000182  ...        0.020717               50\n","..            ...           ...  ...             ...              ...\n","95       0.002688      0.000077  ...        0.011232               55\n","96       0.002676      0.000064  ...        0.010256               72\n","97       0.002735      0.000035  ...        0.010256               72\n","98       0.002656      0.000038  ...        0.010256               72\n","99       0.002649      0.000040  ...        0.011232               55\n","\n","[100 rows x 17 columns]"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"id":"AlICozcIUNGb","colab":{"base_uri":"https://localhost:8080/","height":279},"executionInfo":{"status":"ok","timestamp":1603672331137,"user_tz":180,"elapsed":853,"user":{"displayName":"Martin Iglesias","photoUrl":"https://lh5.googleusercontent.com/-8Fjp34ixMNs/AAAAAAAAAAI/AAAAAAAAAdQ/B0KVwFNI_pc/s64/photo.jpg","userId":"09692603429409562435"}},"outputId":"6d47836b-0eed-47fc-aba0-4f599ff81cd0"},"source":["#Predecimos en los datos de test\n","prediction = model.predict(X_test)\n","\n","# Matriz de Confusion\n","cm = confusion_matrix(y_test,prediction)\n","print(\"Matriz de confusi칩n:\")\n","print(cm)\n","print('Exactitud:', accuracy_score(y_test, prediction))\n","\n","# Reporte de Clasificacion\n","report = classification_report(y_test, prediction)\n","print(\"Reporte de Clasificaci칩n:\")\n","print(report)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Matriz de confusi칩n:\n","[[52  2]\n"," [ 7 82]]\n","Exactitud: 0.9370629370629371\n","Reporte de Clasificaci칩n:\n","              precision    recall  f1-score   support\n","\n","         0.0       0.88      0.96      0.92        54\n","         1.0       0.98      0.92      0.95        89\n","\n","    accuracy                           0.94       143\n","   macro avg       0.93      0.94      0.93       143\n","weighted avg       0.94      0.94      0.94       143\n","\n"],"name":"stdout"}]}]}